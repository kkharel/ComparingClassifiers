# ComparingClassifiers
This project compares the performance of the classifiers, namely K Nearest Neighbor, Logistic Regression, Decision Trees, Support Vector Machines, Random Forest &amp; XGBoost.  We will utilize a dataset related to marketing bank products over the telephone.

The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The classification goal is to predict if the client will subscribe a term deposit (variable y).

Link to Notebook: https://nbviewer.org/github/kkharel/ComparingClassifiers/blob/main/Classifier_Comparison.ipynb

After applying SMOTE-NC to address class imbalance, model performance improved across the board, particularly in terms of generalization to the minority class. Notably, Random Forest and XGBoost achieved the highest test accuracies, while maintaining perfect or near-perfect training accuracy, suggesting strong learning without significant overfitting. The Decision Tree model also showed robust performance, with a perfect training accuracy and a competitive test accuracy. Decision tree is still overfitting the data. Meanwhile, SVC maintained consistent performance post-balancing, though it incurred higher memory usage. Logistic Regression and KNN had slightly lower test accuracies, but still showed reasonable improvements compared to pre-balancing results. Overall, the use of SMOTE-NC helped mitigate the impact of class imbalance, leading to more reliable predictions of term deposit subscriptions, particularly for models like Random Forest and XGBoost that are well-suited to structured tabular data.

The classification report after applying SMOTE-NC reveals substantial improvements in the models’ ability to detect the minority class—clients who subscribed to a term deposit (class 1). While precision for class 1 remains moderate across most models, recall shows notable gains, indicating enhanced sensitivity to positive cases. Logistic Regression, for instance, achieved a recall of 81%, up from pre-balancing levels, with a corresponding F1-score of 0.59, highlighting its improved balance between precision and recall. Similarly, SVC showed the highest recall of 79% for class 1, with a solid F1-score of 0.61, while maintaining strong overall performance (weighted F1-score: 0.89). XGBoost and Random Forest continued to excel, both yielding F1-scores above 0.60 for the minority class, with macro and weighted averages suggesting balanced performance across both classes. KNN, despite its simplicity, showed a significant recall improvement to 73%, though with lower precision. These results demonstrate the effectiveness of SMOTE-NC in enhancing the model’s responsiveness to term deposit subscribers, a critical outcome in a business context where identifying potential customers is essential

The normalized Area Under the Lift Curve (AULC) scores demonstrate that the Decision Tree and KNN models lead performance with an AULC of 0.85, indicating their superior ability to rank clients by likelihood of term deposit subscription. Logistic Regression also performs competitively with an AULC of 0.82, while SVC, Random Forest, and XGBoost share a slightly lower but comparable AULC of 0.81. These results suggest that, despite differences in model complexity, simpler models like Decision Tree and KNN can provide effective lift for targeted marketing campaigns, making them strong candidates for practical deployment when prioritizing client conversion likelihood.

This model is well-suited for marketing applications where maximizing customer conversion is key. A higher recall ensures that more potential subscribers are flagged for follow-up, which is typically more acceptable than missing likely subscribers (false negatives). As a next step, applying PolynomialFeatures to the numeric variables before oversampling could help the model capture non-linear patterns in client behavior, potentially improving subscription predictions further. However, note that Poly Features do not apply for tree based models.


